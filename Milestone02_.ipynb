{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Split prepared data from Milestone 1 into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read two data files\n",
    "df = pd.read_excel('secom.xlsx',header= None)\n",
    "label = pd.read_excel('secom_labels.xlsx', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>time</th>\n",
       "      <th>sensor_0</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_580</th>\n",
       "      <th>sensor_581</th>\n",
       "      <th>sensor_582</th>\n",
       "      <th>sensor_583</th>\n",
       "      <th>sensor_584</th>\n",
       "      <th>sensor_585</th>\n",
       "      <th>sensor_586</th>\n",
       "      <th>sensor_587</th>\n",
       "      <th>sensor_588</th>\n",
       "      <th>sensor_589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19/07/2008 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                 time  sensor_0  sensor_1   sensor_2   sensor_3  \\\n",
       "0     -1  19/07/2008 11:55:00   3030.93   2564.00  2187.7333  1411.1265   \n",
       "1     -1  19/07/2008 12:32:00   3095.78   2465.14  2230.4222  1463.6606   \n",
       "2      1  19/07/2008 13:17:00   2932.61   2559.94  2186.4111  1698.0172   \n",
       "3     -1  19/07/2008 14:43:00   2988.72   2479.90  2199.0333   909.7926   \n",
       "4     -1  19/07/2008 15:22:00   3032.24   2502.87  2233.3667  1326.5200   \n",
       "\n",
       "   sensor_4  sensor_5  sensor_6  sensor_7  ...  sensor_580  sensor_581  \\\n",
       "0    1.3602     100.0   97.6133    0.1242  ...         NaN         NaN   \n",
       "1    0.8294     100.0  102.3433    0.1247  ...      0.0060    208.2045   \n",
       "2    1.5102     100.0   95.4878    0.1241  ...      0.0148     82.8602   \n",
       "3    1.3204     100.0  104.2367    0.1217  ...      0.0044     73.8432   \n",
       "4    1.5334     100.0  100.3967    0.1235  ...         NaN         NaN   \n",
       "\n",
       "   sensor_582  sensor_583  sensor_584  sensor_585  sensor_586  sensor_587  \\\n",
       "0      0.5005      0.0118      0.0035      2.3630         NaN         NaN   \n",
       "1      0.5019      0.0223      0.0055      4.4447      0.0096      0.0201   \n",
       "2      0.4958      0.0157      0.0039      3.1745      0.0584      0.0484   \n",
       "3      0.4990      0.0103      0.0025      2.0544      0.0202      0.0149   \n",
       "4      0.4800      0.4766      0.1045     99.3032      0.0202      0.0149   \n",
       "\n",
       "   sensor_588  sensor_589  \n",
       "0         NaN         NaN  \n",
       "1      0.0060    208.2045  \n",
       "2      0.0148     82.8602  \n",
       "3      0.0044     73.8432  \n",
       "4      0.0044     73.8432  \n",
       "\n",
       "[5 rows x 592 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge two data sets\n",
    "label.columns = ['class', 'time']\n",
    "df = df.add_prefix('sensor_')\n",
    "data = pd.concat([label,df], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataset, impute missing values. \n",
    "\n",
    "First, check whether target attribute has NaN, if yes, drop NaN in class, since imputing class label is not that reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(data['class'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target varibles have no NaN. \n",
    "\n",
    "Check whether time_stamp has NaN. Again, this one is not reasonable to impute values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(data['time'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in time_stamp.\n",
    "\n",
    "Next step, we can loop through each column, and impute missing values with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,590):\n",
    "    name = 'sensor_'+str(i) \n",
    "    HasNan =data[name].isnull()\n",
    "    if sum(HasNan) > 0:\n",
    "        data.loc[HasNan, name] =  np.mean(data[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is cleaned with missing values.\n",
    "\n",
    "There is no more missing values.  Let's take care of time_stamp data. Time_stamp data is a little bit messy, has different kind of format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time_stamp to datetime format\n",
    "data['time'] = pd.to_datetime(data['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is cleaned and ready for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle class imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SMOTE package\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({-1: 1463, 1: 104})\n",
      "Resampled dataset shape Counter({-1: 1092, 1: 1092})\n"
     ]
    }
   ],
   "source": [
    "# handle imbalance data set using SMOTE\n",
    "X = data.iloc[:,2:]\n",
    "y= data['class']\n",
    "\n",
    "#split data first, then apply SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split (X,y, random_state =0)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "sm = SMOTE(random_state=0)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a decision tree model that detects faulty products\n",
    "### 3. Build an ensemble model that detects faulty products\n",
    "### 4. Build an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt = DecisionTreeClassifier().fit(X_res, y_res)\n",
    "dt_pred = dt.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier().fit(X_res, y_res)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "svc = SVC().fit(X_res, y_res)\n",
    "svc_pred = svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 .Evaluate all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "def roc (pred, clf):\n",
    "    roc = roc_auc_score(y_test, pred)\n",
    "    print ('roc for ', clf, ' is: %.2f'%roc)\n",
    "\n",
    "def f1 (pred, clf):\n",
    "    f1 = f1_score (y_test, pred)\n",
    "    print ('F1 for ', clf, ' is: %.2f'%f1)\n",
    "\n",
    "def recall (pred, clf):\n",
    "    sco = recall_score (y_test, pred)\n",
    "    print ('Recall for ', clf, ' is: %.2f'%sco)\n",
    "    \n",
    "def precision (pred, clf):\n",
    "    sco = precision_score (y_test, pred)\n",
    "    print ('Precision for ', clf, ' is: %.2f'%sco)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  Decision tree  is: 0.58\n",
      "F1 for  Decision tree  is: 0.17\n",
      "Recall for  Decision tree  is: 0.29\n",
      "Precision for  Decision tree  is: 0.12\n"
     ]
    }
   ],
   "source": [
    "roc(dt_pred, 'Decision tree')\n",
    "f1(dt_pred, 'Decision tree')\n",
    "recall(dt_pred, 'Decision tree')\n",
    "precision(dt_pred, 'Decision tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  Random Forest  is: 0.49\n",
      "F1 for  Random Forest  is: 0.00\n",
      "Recall for  Random Forest  is: 0.00\n",
      "Precision for  Random Forest  is: 0.00\n"
     ]
    }
   ],
   "source": [
    "roc(rf_pred, 'Random Forest')\n",
    "f1(rf_pred, 'Random Forest')\n",
    "recall(rf_pred, 'Random Forest')\n",
    "precision(rf_pred, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  SVC  is: 0.45\n",
      "F1 for  SVC  is: 0.07\n",
      "Recall for  SVC  is: 0.33\n",
      "Precision for  SVC  is: 0.04\n"
     ]
    }
   ],
   "source": [
    "roc(svc_pred, 'SVC')\n",
    "f1(svc_pred, 'SVC')\n",
    "recall(svc_pred, 'SVC')\n",
    "precision(svc_pred, 'SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default model, performance of decision tree, random forest and SVC are all bad. Hyperparameters need to be optimized. GridSearchCV will beused to search for optimized paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All default models perform very poor, let's try to use GridSearchCV to optimize the parameters (use f1 as scoring method)\n",
    "\n",
    "##### GridSearch on DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best decision tree model: {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_para = {'max_depth': np.arange(1,5),'min_samples_leaf': np.arange(1,5), 'criterion':['gini', 'entropy']}\n",
    "dt_cv = GridSearchCV(DecisionTreeClassifier(), dt_para, scoring = 'f1').fit(X_res, y_res)\n",
    "print('Best decision tree model:', dt_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  Decision Tree  is: 0.59\n",
      "F1 for  Decision Tree  is: 0.14\n",
      "Recall for  Decision Tree  is: 0.48\n",
      "Precision for  Decision Tree  is: 0.08\n"
     ]
    }
   ],
   "source": [
    "dt_cv = DecisionTreeClassifier(criterion ='gini', max_depth = 4, min_samples_leaf= 3).fit(X_res, y_res)\n",
    "dt_cv_pred = dt_cv.predict(X_test)\n",
    "roc(dt_cv_pred, 'Decision Tree')\n",
    "f1(dt_cv_pred, 'Decision Tree')\n",
    "recall(dt_cv_pred, 'Decision Tree')\n",
    "precision(dt_cv_pred, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall perfornance for the best decision tree model does not improve a lot, suggesting that search range might not have the optimal parameters. \n",
    "\n",
    "###### GridSearch on RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best random forest model: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "rf_para = {'n_estimators': [80,100,120], \n",
    "           'max_depth': np.arange(1,5),\n",
    "           'min_samples_leaf': np.arange(1,5), \n",
    "           'criterion':['gini', 'entropy']}\n",
    "rf_cv =GridSearchCV(RandomForestClassifier(), rf_para, scoring ='f1').fit(X_res, y_res)\n",
    "print('Best random forest model:', rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  Random Forest  is: 0.64\n",
      "F1 for  Random Forest  is: 0.21\n",
      "Recall for  Random Forest  is: 0.43\n",
      "Precision for  Random Forest  is: 0.14\n"
     ]
    }
   ],
   "source": [
    "rf_cv = RandomForestClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 1, n_estimators=80).fit(X_res, y_res)\n",
    "rf_cv_pred = rf_cv.predict(X_test)\n",
    "roc(rf_cv_pred, 'Random Forest')\n",
    "f1(rf_cv_pred, 'Random Forest')\n",
    "recall(rf_cv_pred, 'Random Forest')\n",
    "precision(rf_cv_pred, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall perfornance for the best random forest model improves a lot, and its performance is better than the best decision tree model.\n",
    "\n",
    "##### GridSearch on SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: {'C': 1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "cv_params = {'kernel':['rbf'], 'C':[0.001,0.01,1], }\n",
    "svc_cv = GridSearchCV(SVC(), cv_params, scoring ='f1').fit(X_res, y_res)\n",
    "print('Best model:', svc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc for  SVC  is: 0.45\n",
      "F1 for  SVC  is: 0.07\n",
      "Recall for  SVC  is: 0.33\n",
      "Precision for  SVC  is: 0.04\n"
     ]
    }
   ],
   "source": [
    "svc_cv = SVC(kernel = 'rbf', C=1).fit(X_res, y_res)\n",
    "# svc_cv = SVC().fit(X_res, y_res)\n",
    "svc_cv_pred = svc_cv.predict(X_test)\n",
    "roc(svc_cv_pred, 'SVC')\n",
    "f1(svc_cv_pred, 'SVC')\n",
    "recall(svc_cv_pred, 'SVC')\n",
    "precision(svc_cv_pred, 'SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model using GridSearchCV is rbf with C=1,which surprisingly is the same as the default model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Solicit specific feedback on your code \n",
    "\n",
    "1. How to determine which model to be used in general? \n",
    "2. How to determine which metrics to be used when comparing different models? I understand that stick with one metrics is necessary, but I am not sure should I choose roc or f1, or others?\n",
    "3. What can I if the performance is still not good after GridSearchCV?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
